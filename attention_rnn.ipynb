{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaged-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.utils import shuffle\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "#import Levenshtein\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cellular-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "different-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "twelve-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(rootpath + \"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modified-thanks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (2424186, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000011a64c74</td>\n",
       "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
       "      <td>data/train_inception/0/0/0/000011a64c74.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000019cc0cd2</td>\n",
       "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
       "      <td>data/train_inception/0/0/0/000019cc0cd2.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000252b6d2b</td>\n",
       "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
       "      <td>data/train_inception/0/0/0/0000252b6d2b.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000026b49b7e</td>\n",
       "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
       "      <td>data/train_inception/0/0/0/000026b49b7e.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000026fc6c36</td>\n",
       "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
       "      <td>data/train_inception/0/0/0/000026fc6c36.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                              InChI  \\\n",
       "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...   \n",
       "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...   \n",
       "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...   \n",
       "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...   \n",
       "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...   \n",
       "\n",
       "                                     file_path  \n",
       "0  data/train_inception/0/0/0/000011a64c74.npy  \n",
       "1  data/train_inception/0/0/0/000019cc0cd2.npy  \n",
       "2  data/train_inception/0/0/0/0000252b6d2b.npy  \n",
       "3  data/train_inception/0/0/0/000026b49b7e.npy  \n",
       "4  data/train_inception/0/0/0/000026fc6c36.npy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_train_file_path(image_id):\n",
    "    return \"data/train_inception/{}/{}/{}/{}.npy\".format(\n",
    "        image_id[0], image_id[1], image_id[2], image_id \n",
    "    )\n",
    "\n",
    "train['file_path'] = train['image_id'].apply(get_train_file_path)\n",
    "\n",
    "print(f'train.shape: {train.shape}')\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "injured-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_label(InChI):\n",
    "    return \"<{}>\".format(InChI.replace(\"InChI=1S/\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "false-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"InChI_clean\"] = train[\"InChI\"].apply(preprocess_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imperial-salvation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>file_path</th>\n",
       "      <th>InChI_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000011a64c74</td>\n",
       "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
       "      <td>data/train_inception/0/0/0/000011a64c74.npy</td>\n",
       "      <td>&lt;C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12(13)11(4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000019cc0cd2</td>\n",
       "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
       "      <td>data/train_inception/0/0/0/000019cc0cd2.npy</td>\n",
       "      <td>&lt;C21H30O4/c1-12(22)25-14-6-8-20(2)13(10-14)11-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000252b6d2b</td>\n",
       "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
       "      <td>data/train_inception/0/0/0/0000252b6d2b.npy</td>\n",
       "      <td>&lt;C24H23N5O4/c1-14-13-15(7-8-17(14)28-12-10-20(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000026b49b7e</td>\n",
       "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
       "      <td>data/train_inception/0/0/0/000026b49b7e.npy</td>\n",
       "      <td>&lt;C17H24N2O4S/c1-12(20)18-13(14-7-6-10-24-14)11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000026fc6c36</td>\n",
       "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
       "      <td>data/train_inception/0/0/0/000026fc6c36.npy</td>\n",
       "      <td>&lt;C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7-8)5-2-3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id                                              InChI  \\\n",
       "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...   \n",
       "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...   \n",
       "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...   \n",
       "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...   \n",
       "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...   \n",
       "\n",
       "                                     file_path  \\\n",
       "0  data/train_inception/0/0/0/000011a64c74.npy   \n",
       "1  data/train_inception/0/0/0/000019cc0cd2.npy   \n",
       "2  data/train_inception/0/0/0/0000252b6d2b.npy   \n",
       "3  data/train_inception/0/0/0/000026b49b7e.npy   \n",
       "4  data/train_inception/0/0/0/000026fc6c36.npy   \n",
       "\n",
       "                                         InChI_clean  \n",
       "0  <C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12(13)11(4...  \n",
       "1  <C21H30O4/c1-12(22)25-14-6-8-20(2)13(10-14)11-...  \n",
       "2  <C24H23N5O4/c1-14-13-15(7-8-17(14)28-12-10-20(...  \n",
       "3  <C17H24N2O4S/c1-12(20)18-13(14-7-6-10-24-14)11...  \n",
       "4  <C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7-8)5-2-3...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "binary-uganda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizing\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"q\", filters=' ', lower=False, char_level=True)\n",
    "tokenizer.fit_on_texts(train[\"InChI_clean\"].values)\n",
    "tokenizer.word_index['q'] = 0\n",
    "tokenizer.index_word[0] = 'q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "meaningful-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max(tokenizer.index_word.keys())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sustainable-smith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abandoned-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token = tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences(train[\"InChI_clean\"].values), padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "exotic-buffalo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 18,  2, ...,  0,  0,  0],\n",
       "       [19, 18,  4, ...,  0,  0,  0],\n",
       "       [19, 18,  4, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [19, 18,  4, ...,  0,  0,  0],\n",
       "       [19, 18,  2, ...,  0,  0,  0],\n",
       "       [19, 18,  2, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "functioning-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2424186, 396)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fatty-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = train[\"file_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "balanced-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "small-alfred",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2424186"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "steady-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.from_tensor_slices((paths, train_token))\n",
    "list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n",
    "\n",
    "#for f in list_ds.take(5):\n",
    "#    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conventional-basketball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1939349\n",
      "484837\n"
     ]
    }
   ],
   "source": [
    "val_size = int(image_count * 0.2)\n",
    "train_ds = list_ds.skip(val_size)\n",
    "val_ds = list_ds.take(val_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "blond-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = tf.data.experimental.cardinality(train_ds).numpy() // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "criminal-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30302"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "chronic-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "autotune = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "figured-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(image_path, label):\n",
    "    img_tensor = np.load(image_path.decode('utf-8'))\n",
    "    return img_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "choice-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "list_ds = list_ds.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aggressive-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(ds, autotune=tf.data.AUTOTUNE):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=BUFFER_SIZE)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=autotune)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mature-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = configure_for_performance(train_ds, autotune=autotune)\n",
    "val_ds = configure_for_performance(val_ds, autotune=autotune)\n",
    "list_ds = configure_for_performance(list_ds, autotune=autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "offshore-notion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                             self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # score shape == (batch_size, 64, 1)\n",
    "        # This gives you an unnormalized score for each image feature.\n",
    "        score = self.V(attention_hidden_layer)\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dental-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "moderate-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    " \n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sudden-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "twelve-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "entire-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/attention_rnn_train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "instrumental-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "    # restoring the latest checkpoint in checkpoint_path\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "patient-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dental-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-sapphire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0/30302 Loss 1.1894\n",
      "Epoch 1 Batch 100/30302 Loss 0.6081\n",
      "Epoch 1 Batch 200/30302 Loss 0.5357\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_ds):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {}/{} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, num_steps, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-northwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-diameter",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "concerned-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(rootpath + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "integrated-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (1616107, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>InChI</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000d2a601c</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>data/bms-molecular-translation/test/0/0/0/0000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00001f7fc849</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>data/bms-molecular-translation/test/0/0/0/0000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000037687605</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>data/bms-molecular-translation/test/0/0/0/0000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00004b6d55b6</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>data/bms-molecular-translation/test/0/0/0/0000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00004df0fe53</td>\n",
       "      <td>InChI=1S/H2O/h1H2</td>\n",
       "      <td>data/bms-molecular-translation/test/0/0/0/0000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id              InChI  \\\n",
       "0  00000d2a601c  InChI=1S/H2O/h1H2   \n",
       "1  00001f7fc849  InChI=1S/H2O/h1H2   \n",
       "2  000037687605  InChI=1S/H2O/h1H2   \n",
       "3  00004b6d55b6  InChI=1S/H2O/h1H2   \n",
       "4  00004df0fe53  InChI=1S/H2O/h1H2   \n",
       "\n",
       "                                           file_path  \n",
       "0  data/bms-molecular-translation/test/0/0/0/0000...  \n",
       "1  data/bms-molecular-translation/test/0/0/0/0000...  \n",
       "2  data/bms-molecular-translation/test/0/0/0/0000...  \n",
       "3  data/bms-molecular-translation/test/0/0/0/0000...  \n",
       "4  data/bms-molecular-translation/test/0/0/0/0000...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_train_file_path(image_id):\n",
    "    return rootpath + \"test/{}/{}/{}/{}.png\".format(\n",
    "        image_id[0], image_id[1], image_id[2], image_id \n",
    "    )\n",
    "\n",
    "test['file_path'] = test['image_id'].apply(get_train_file_path)\n",
    "\n",
    "print(f'train.shape: {test.shape}')\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "needed-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "practical-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(img_tensor_val):\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "traditional-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_id(filepath):\n",
    "    return filepath.split(\"/\")[-1].replace(\".png\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for path in test['file_path']:\n",
    "    img = load_image(path)\n",
    "    img_tensor_val = image_features_extract_model(img)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "    prediction = evaluate(img_tensor_val)\n",
    "    result.append({\"image_id\": get_img_id(path), \"InChI\": prediction.replace(\"<\", \"InChI=1S/\").replace(\">\",\"\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"results_attention_rnn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-story",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
